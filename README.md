## Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View (ICLR 2025)


Hi, I'm currently sorting out the readme part, which will soon be posted here; thanks.
---

CogMir is an open-ended Multi-LLM Agents framework designed to explore how large language models (LLMs) mirror human cognitive biases and exhibit irrational yet prosocial decision-making. Our research highlights the potential of using systematic hallucination properties in LLMs to better understand and enhance their social intelligence.

If you find our work helpful or thought-provoking, please cite us at:

```bibtex
@inproceedings{
  liu2025exploring,
  title={Exploring Prosocial Irrationality for {LLM} Agents: A Social Cognition View},
  author={Xuan Liu and Jie Zhang and Haoyang Shang and Song Guo and Chengxu Yang and Quanyan Zhu},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=u8VOQVzduP}
}
